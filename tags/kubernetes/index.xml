<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Mya Pitzeruse</title>
    <link>https://mjpitz.com/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Mya Pitzeruse</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Dec 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mjpitz.com/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Renovate your GitOps</title>
      <link>https://mjpitz.com/blog/2020/12/03/renovate-your-gitops/</link>
      <pubDate>Thu, 03 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/12/03/renovate-your-gitops/</guid>
      <description>&lt;p&gt;Every engineering organization struggles to stay up to date with the latest versions of applications they run.
When an organization deploys an open source project, their versions start to drift from day one.
The longer a project runs without an update, the more likely it is to contain a vulnerability.
To help applications stay on top of library versions, the project &lt;a href=&#34;https://github.com/renovatebot/&#34;&gt;Renovate&lt;/a&gt; was developed.
Renovate works by parsing manifest files (like &lt;code&gt;package.json&lt;/code&gt; and &lt;code&gt;go.mod&lt;/code&gt;) and checking for newer versions of libraries.
When Renovate discovers an update, it submits a pull request with the newer version to the project.&lt;/p&gt;
&lt;p&gt;Recently, I noticed Renovate submit pull requests for dependencies in my &lt;a href=&#34;https://helm.sh/&#34;&gt;Helm&lt;/a&gt; v3 charts.
This gave me an idea.
What if Renovate could automatically manage something like a GitOps repository?
This means organizations would no longer need to tediously query for newer versions of applications.
Instead, they&amp;rsquo;d automatically receive a pull request when an update becomes available.
In this blog post, I demonstrate how to set this up for an &lt;a href=&#34;https://github.com/argoproj/argo-cd/&#34;&gt;ArgoCD&lt;/a&gt; GitOps repository.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Running a Service Mesh on Raspberry Pis</title>
      <link>https://mjpitz.com/blog/2020/11/23/service-mesh-rpi/</link>
      <pubDate>Mon, 23 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/11/23/service-mesh-rpi/</guid>
      <description>&lt;p&gt;Many people have asked how to support deploying service mesh to Raspberry Pis.
It wasn&amp;rsquo;t until September that this started to be possible.
&lt;a href=&#34;https://github.com/linkerd/linkerd2/releases/tag/stable-2.9.0&#34;&gt;Linkerd&lt;/a&gt; recently released support for arm64, but has had support for it in edge versions since August.
Many &lt;a href=&#34;https://envoyproxy.io/&#34;&gt;envoy&lt;/a&gt; based service mesh have been blocked by support for an arm-compatible envoy image.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.consul.io/&#34;&gt;Consul&lt;/a&gt; is a powerful service discovery and configuration management tool from &lt;a href=&#34;https://www.hashicorp.com/&#34;&gt;Hashicorp&lt;/a&gt;.
It has a long history of supporting a variety of execution platforms, operating systems, and architectures.
In 1.2, Hashicorp introduced &lt;a href=&#34;https://www.consul.io/docs/connect&#34;&gt;Consul Connect&lt;/a&gt;, an envoy based service mesh integration.
This allows Consul to control and direct clients in the service mesh data plane.&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll demonstrate how to deploy Consul to support a service mesh on Raspberry Pis.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Adventures in Path Based Routing</title>
      <link>https://mjpitz.com/blog/2020/11/10/path-based-routing-k8s/</link>
      <pubDate>Tue, 10 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/11/10/path-based-routing-k8s/</guid>
      <description>&lt;p&gt;Path based routing can be an extremely useful feature.
It enables you to serve a single page app and an API on the same domain.
This can often be helpful when starting a project, but don&amp;rsquo;t want to handle things like &lt;a href=&#34;https://developer.mozilla.org/en-US/docs/Web/HTTP/CORS&#34;&gt;cross-origin resource sharing&lt;/a&gt;.
In a recent project, I wanted to split traffic between a static site hosted on GitHub (or S3) and an API running in the cluster.
In this post, I&amp;rsquo;ll demonstrate some less common approaches to path based routing using &lt;a href=&#34;https://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; resources.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Local Ingress Domains for your Kind Cluster</title>
      <link>https://mjpitz.com/blog/2020/10/21/local-ingress-domains-kind/</link>
      <pubDate>Wed, 21 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/10/21/local-ingress-domains-kind/</guid>
      <description>&lt;p&gt;Tools like &lt;a href=&#34;https://minikube.sigs.k8s.io/docs/&#34;&gt;minikube&lt;/a&gt; and &lt;a href=&#34;https://kind.sigs.k8s.io/&#34;&gt;kind&lt;/a&gt; make it easy to get a &lt;a href=&#34;https://kubernetes.io/&#34;&gt;kubernetes&lt;/a&gt; cluster up and running locally.
Unfortunately these tools are limited in their capabilities, namely a lack of load balancer support.
As a result, the community developed solutions like &lt;a href=&#34;https://github.com/txn2/kubefwd&#34;&gt;kubefwd&lt;/a&gt; and &lt;code&gt;minikube tunnel&lt;/code&gt; to expose services.
While this approach works, keeping a dedicated terminal open during development can be tedeous.
In this post, I show how to set up an ingress controller in a kind cluster and pair it with a private, locally addressable domain.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Reducing cost on DigitalOcean</title>
      <link>https://mjpitz.com/blog/2020/08/03/digitalocean-setup/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/08/03/digitalocean-setup/</guid>
      <description>&lt;p&gt;In a &lt;a href=&#34;https://twitter.com/_mjpitz_/status/1290258134590603269&#34;&gt;Twitter thread&lt;/a&gt; between &lt;a href=&#34;https://twitter.com/vitobotta&#34;&gt;Vito Botta&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/alexellisuk&#34;&gt;Alex Ellis&lt;/a&gt;, and &lt;a href=&#34;https://twitter.com/_mjpitz_&#34;&gt;myself&lt;/a&gt;,
we talked about how expensive &lt;a href=&#34;https://digitalocean.com/&#34;&gt;DigitalOcean&lt;/a&gt; can be for personal projects.
You often start off small with just a cluster for compute.
Eventually you need a database to store your user&amp;rsquo;s information.
As time goes on, these needs only continue to grow.
In this post, I share some cost-saving techniques I&amp;rsquo;ve used to reduce my bill.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Home Lab: 1 year later</title>
      <link>https://mjpitz.com/blog/2020/04/20/k3s-rpi-year1/</link>
      <pubDate>Mon, 20 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/04/20/k3s-rpi-year1/</guid>
      <description>&lt;p&gt;Last year, I wrote a series of blog posts covering the set-up of my home lab.
The &lt;a href=&#34;https://mjpitz.com/blog/2019/04/10/k8s-k3s-rpi-oh-my/&#34;&gt;first&lt;/a&gt; post was on my decision to run &lt;a href=&#34;https://k3s.io/&#34;&gt;Rancher&amp;rsquo;s k3s&lt;/a&gt; on my &lt;a href=&#34;https://www.raspberrypi.org/&#34;&gt;Raspberry Pis&lt;/a&gt;.
Since then, I&amp;rsquo;ve made a few modifications to how its all managed.
In this post, I discuss some of these changes I made over the last year.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Building deps.cloud</title>
      <link>https://mjpitz.com/blog/2020/01/24/building-depscloud/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/01/24/building-depscloud/</guid>
      <description>&lt;p&gt;Over the last year, I&amp;rsquo;ve been heavily working on &lt;a href=&#34;https://deps.cloud&#34;&gt;deps.cloud&lt;/a&gt;.
deps.cloud draws it&amp;rsquo;s inspiration from a project that I worked on at &lt;a href=&#34;https://indeed.com&#34;&gt;Indeed.com&lt;/a&gt;.
Since it&amp;rsquo;s original inception, there had been a heavy push to move it into the open source space.
In this post, I&amp;rsquo;ll discuss the process and rationale I applied as I rewrote this project in the open.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Checking Service Dependencies in Kubernetes</title>
      <link>https://mjpitz.com/blog/2019/10/17/kubernetes-service-precheck/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2019/10/17/kubernetes-service-precheck/</guid>
      <description>&lt;p&gt;Back in July, I found myself needing to better coordinate deployments of my applications to &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;.
After searching around, I found many ways that people where trying to solve this problem.
Some used shell scripts to apply multiple YAML files with a fixed time sleep between them.
Others used shell scripts and tailed the rollout using &lt;code&gt;kubectl rollout status -w&lt;/code&gt;.
Now, I manage a lot of my deployments using &lt;a href=&#34;https://www.weave.works/technologies/gitops/&#34;&gt;GitOps&lt;/a&gt; and &lt;a href=&#34;https://github.com/fluxcd/flux&#34;&gt;Flux&lt;/a&gt;.
So leveraging these shell scripts to manage my rollouts into clusters wasn&amp;rsquo;t really an option.&lt;/p&gt;
&lt;p&gt;It wasn&amp;rsquo;t until I came across &lt;a href=&#34;https://us.alibabacloud.com&#34;&gt;Alibaba Cloud&amp;rsquo;s&lt;/a&gt; blog post on &lt;a href=&#34;https://www.alibabacloud.com/blog/kubernetes-demystified-solving-service-dependencies_594110&#34;&gt;solving service dependencies&lt;/a&gt; that I felt like I had something to work with.
The article described two techniques.
The first was inspecting dependencies within the application itself.
At Indeed, we leverage our &lt;a href=&#34;http://github.com/indeedeng/status&#34;&gt;status&lt;/a&gt; library to do this.
The second was to enable services to be checked, independent of the application.&lt;/p&gt;
&lt;p&gt;In this post, I’ll demonstrate how to use my &lt;a href=&#34;https://hub.docker.com/r/mjpitz/service-precheck&#34;&gt;service-precheck&lt;/a&gt; initialization container (built off of the Alibaba blog post) to ensure upstream systems are up before attempting to start a downstream system.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>k3s on Raspberry Pi</title>
      <link>https://mjpitz.com/blog/2019/04/10/k8s-k3s-rpi-oh-my/</link>
      <pubDate>Wed, 10 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2019/04/10/k8s-k3s-rpi-oh-my/</guid>
      <description>&lt;p&gt;Over the last few days, I&amp;rsquo;ve been revisiting &lt;a href=&#34;https://kubernetes.io&#34;&gt;Kubernetes&lt;/a&gt; on my Raspberry Pi cluster.
I hope to share what I learned in the process and some of the tooling that I discovered along the way.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>