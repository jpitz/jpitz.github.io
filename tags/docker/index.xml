<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>docker on Mya Pitzeruse</title>
    <link>https://mjpitz.com/tags/docker/</link>
    <description>Recent content in docker on Mya Pitzeruse</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Nov 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://mjpitz.com/tags/docker/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Docker Registry Setup</title>
      <link>https://mjpitz.com/blog/2020/11/03/registry-123/</link>
      <pubDate>Tue, 03 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/11/03/registry-123/</guid>
      <description>&lt;p&gt;DockerHub&amp;rsquo;s &lt;a href=&#34;https://docs.docker.com/docker-hub/download-rate-limit/&#34;&gt;impending download rate limit&lt;/a&gt; presents an interesting challenge for some.
From hobbyists to open core ecosystems, projects are trying to find ways insulate their users.
For my projects, I chose to deploy a simple registry mirror.
One nice thing about this project is that the system is largely stateless (and cheap to run).
The &lt;code&gt;docker-registry&lt;/code&gt; and &lt;code&gt;docker-auth&lt;/code&gt; projects are pretty easy to horizontally scale.
The only stateful system you really need to manage is a cache (which isn&amp;rsquo;t mission critical).
While &lt;a href=&#34;https://goharbor.io/&#34;&gt;Harbor&lt;/a&gt; was appealing, it had a lot more overhead than what I needed.
In this post, I&amp;rsquo;ll walk you through my deployment.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Building deps.cloud</title>
      <link>https://mjpitz.com/blog/2020/01/24/building-depscloud/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2020/01/24/building-depscloud/</guid>
      <description>&lt;p&gt;Over the last year, I&amp;rsquo;ve been heavily working on &lt;a href=&#34;https://deps.cloud&#34;&gt;deps.cloud&lt;/a&gt;.
deps.cloud draws it&amp;rsquo;s inspiration from a project that I worked on at &lt;a href=&#34;https://indeed.com&#34;&gt;Indeed.com&lt;/a&gt;.
Since it&amp;rsquo;s original inception, there had been a heavy push to move it into the open source space.
In this post, I&amp;rsquo;ll discuss the process and rationale I applied as I rewrote this project in the open.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Checking Service Dependencies in Kubernetes</title>
      <link>https://mjpitz.com/blog/2019/10/17/kubernetes-service-precheck/</link>
      <pubDate>Thu, 17 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2019/10/17/kubernetes-service-precheck/</guid>
      <description>&lt;p&gt;Back in July, I found myself needing to better coordinate deployments of my applications to &lt;a href=&#34;https://kubernetes.io/&#34;&gt;Kubernetes&lt;/a&gt;.
After searching around, I found many ways that people where trying to solve this problem.
Some used shell scripts to apply multiple YAML files with a fixed time sleep between them.
Others used shell scripts and tailed the rollout using &lt;code&gt;kubectl rollout status -w&lt;/code&gt;.
Now, I manage a lot of my deployments using &lt;a href=&#34;https://www.weave.works/technologies/gitops/&#34;&gt;GitOps&lt;/a&gt; and &lt;a href=&#34;https://github.com/fluxcd/flux&#34;&gt;Flux&lt;/a&gt;.
So leveraging these shell scripts to manage my rollouts into clusters wasn&amp;rsquo;t really an option.&lt;/p&gt;
&lt;p&gt;It wasn&amp;rsquo;t until I came across &lt;a href=&#34;https://us.alibabacloud.com&#34;&gt;Alibaba Cloud&amp;rsquo;s&lt;/a&gt; blog post on &lt;a href=&#34;https://www.alibabacloud.com/blog/kubernetes-demystified-solving-service-dependencies_594110&#34;&gt;solving service dependencies&lt;/a&gt; that I felt like I had something to work with.
The article described two techniques.
The first was inspecting dependencies within the application itself.
At Indeed, we leverage our &lt;a href=&#34;http://github.com/indeedeng/status&#34;&gt;status&lt;/a&gt; library to do this.
The second was to enable services to be checked, independent of the application.&lt;/p&gt;
&lt;p&gt;In this post, Iâ€™ll demonstrate how to use my &lt;a href=&#34;https://hub.docker.com/r/mjpitz/service-precheck&#34;&gt;service-precheck&lt;/a&gt; initialization container (built off of the Alibaba blog post) to ensure upstream systems are up before attempting to start a downstream system.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Using docker-buildx for Multi-architecture Containers</title>
      <link>https://mjpitz.com/blog/2019/05/07/docker-buildx/</link>
      <pubDate>Tue, 07 May 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2019/05/07/docker-buildx/</guid>
      <description>&lt;p&gt;When you build a container image, it&amp;rsquo;s typically only built for one platform (&lt;code&gt;linux&lt;/code&gt;) and one architecture (&lt;code&gt;amd64&lt;/code&gt;).
As the Internet of Things continues to grow, the demand for more &lt;code&gt;arm&lt;/code&gt; images increased as well.
Traditionally, in order to produce an &lt;code&gt;arm&lt;/code&gt; image, you need an &lt;code&gt;arm&lt;/code&gt; device to do the build on.
As a result, most projects wind up missing &lt;code&gt;arm&lt;/code&gt; support.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/moby/buildkit&#34;&gt;BuildKit&lt;/a&gt; provides emulation capabilities that support multi-architecture builds.
With BuildKit, you build container images across multiple architectures concurrently.
This core utility backs &lt;code&gt;docker buildx&lt;/code&gt;, a multi-architecture build utility for docker.
In this post, I&amp;rsquo;ll discuss why you should produce multi-architecture container images and demonstrate how to use &lt;code&gt;docker buildx&lt;/code&gt; to do it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster Monitoring</title>
      <link>https://mjpitz.com/blog/2019/04/21/monitoring-rpi-cluster/</link>
      <pubDate>Sun, 21 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2019/04/21/monitoring-rpi-cluster/</guid>
      <description>&lt;p&gt;In my last few posts, I talked a bit about my at home development cluster.
Due to the flexibility of my cluster, I wanted to provide a monitoring solution that was valuable across each technology I use.
In this post, I discuss how monitoring is setup on my cluster.
I&amp;rsquo;ll walk through setting up each node, the Prometheus server, and the Graphana UI.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Raspberry Pi Cluster Setup</title>
      <link>https://mjpitz.com/blog/2019/04/12/rpi-cluster-setup/</link>
      <pubDate>Fri, 12 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2019/04/12/rpi-cluster-setup/</guid>
      <description>&lt;p&gt;Previously, I talked about the different orchestration technologies that I&amp;rsquo;ve run on my Raspberry Pi cluster.
That post was rather high level and only contained details relating to k3s.
In this post, we&amp;rsquo;ll take a more in depth look at my cluster setup and my management process around it.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Docker Machine DNS Resolution using Consul</title>
      <link>https://mjpitz.com/blog/2016/05/08/docker-machine-dns-resolution-using-consul/</link>
      <pubDate>Sun, 08 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://mjpitz.com/blog/2016/05/08/docker-machine-dns-resolution-using-consul/</guid>
      <description>&lt;p&gt;Developers at &lt;a href=&#34;http://www.indeed.com&#34;&gt;Indeed&lt;/a&gt; have &lt;em&gt;recently&lt;/em&gt; switched over to using docker for local development.
Being one of the earlier adopters, I fell in love with the type of workflow that it enabled.
It allowed me to create seamless environments between both my desktop and portable workstation.
The tooling did this by allowing you to resolve container names as hosts in your web browser.
For example, if I had a web application named &lt;strong&gt;indigo&lt;/strong&gt; running on port 4000, I could go to http://indigo:4000 to access that application.
After a few weeks of enjoying the simplicity of this development workflow, I craved a similar type of environment for some of the larger scale projects that I do at home.
In this blog post, I will cover some of the basics that allowed me to enable this type of development.&lt;/p&gt;</description>
    </item>
    
  </channel>
</rss>